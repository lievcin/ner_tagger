{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras import Model, Input\n",
    "# from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "# from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "import json\n",
    "import functools\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# path = Path(__file__).parent\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\".\")\n",
    "# from datasets.dataset import Dataset\n",
    "\n",
    "# class LSTMCRF:\n",
    "\n",
    "#     def __init__(self, vocabulary_size, labels_size, embeddings_size=50, lstm_units=100, dropout=0.2):\n",
    "#         input_word = Input(shape=(embeddings_size,))\n",
    "#         model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "#         model = SpatialDropout1D(dropout)(model)\n",
    "#         model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "#         out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "#         self.model = Model(input_word, out)\n",
    "#         self.checkpoint = ModelCheckpoint(\"{}/model.h5\".format(path), monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "\n",
    "#     def fit(self, data):\n",
    "#         self.model.compile(optimizer=\"adam\",\n",
    "#             loss=\"sparse_categorical_crossentropy\",\n",
    "#             metrics=[\"accuracy\"])\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = data.Xy\n",
    "\n",
    "#         self.model.fit(\n",
    "#             x=X_train,\n",
    "#             y=y_train,\n",
    "#             validation_data=(X_test,y_test),\n",
    "#             batch_size=32,\n",
    "#             epochs=3,\n",
    "#             callbacks=[self.checkpoint, EarlyStopping()],\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#     def test(self, data):\n",
    "#         _, X, _, y = data.Xy\n",
    "#         self.model.evaluate(X, y)\n",
    "#         n_sequences, sequence_len = X.shape\n",
    "#         predictions = self.model.predict(X)\n",
    "#         flat_predictions = []\n",
    "#         flat_gold = []\n",
    "#         with open(\"{}/predictions.txt\".format(path), \"w\") as f:\n",
    "#             f.write(f\"WORD\\tPREDICTION\\tGOLD STANDARD\\n\")\n",
    "#             for sequence_i in range(n_sequences):\n",
    "#                 f.write(\"\\n\")\n",
    "#                 for word_i in range(sequence_len):\n",
    "#                     word = data.idx2word[X[sequence_i, word_i]]\n",
    "#                     prediction_i = np.argmax(predictions[sequence_i, word_i, :])\n",
    "#                     prediction = data.idx2tag[prediction_i]\n",
    "#                     flat_predictions.append(prediction)\n",
    "#                     truth = data.idx2tag[y[sequence_i, word_i]]\n",
    "#                     flat_gold.append(truth)\n",
    "#                     f.write(f\"{word}\\t{prediction}\\t{truth}\\n\")\n",
    "#         with open(\"{}/metric-report.txt\".format(path), \"w\") as f:\n",
    "#             report = classification_report(flat_gold, flat_predictions)\n",
    "#             f.write(report)\n",
    "\n",
    "#     def save(self):\n",
    "#         # serialize model to JSON\n",
    "#         model_json = self.model.to_json()\n",
    "#         with open(\"{}/model.json\".format(path), \"w\") as json_file:\n",
    "#             json_file.write(model_json)\n",
    "#         # serialize weights to HDF5\n",
    "#         self.model.save_weights(\"{}/model.h5\".format(path))\n",
    "#         print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset = Dataset(path=\"./data/processed_data/gmb-1.0.0.csv\", maxlen=50)\n",
    "#     model = LSTMCRF(dataset.vocabulary_size, dataset.labels_size)\n",
    "#     model.fit(dataset)\n",
    "#     model.test(dataset)\n",
    "#     model.save()\n",
    "#     # print(path)\n",
    "\n",
    "\n",
    "DATADIR = './data/processed_data/gmb'\n",
    "\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "    # Encode in Bytes for TF\n",
    "    words = [w.encode() for w in line_words.strip().split()]\n",
    "    tags = [t.encode() for t in line_tags.strip().split()]\n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "    return (words, len(words)), tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "\n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None], ()), [None])\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    defaults = (('<pad>', 0), 'O')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_shapes=shapes, output_types=types)\n",
    "\n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    dataset = (dataset\n",
    "               .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    input_word = Input(shape=(embeddings_size,))\n",
    "    model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "    model = SpatialDropout1D(dropout)(model)\n",
    "    model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "    out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "    self.model = Model(input_word, out)\n",
    "    self.checkpoint = ModelCheckpoint(\"results/model.h5\", monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, model_dir=model_dir)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = {\n",
    "        \"dim\": 300,\n",
    "        \"dropout\": 0.5,\n",
    "        # \"num_oov_buckets\": 1,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer\": 15000,\n",
    "        \"lstm_size\": 100,\n",
    "        \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "        \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "    }\n",
    "\n",
    "    with Path(\"results/params.json\").open(\"w\") as f:\n",
    "        json.dump(params, f, indent=4, sort_keys=True)\n",
    "\n",
    "    def fwords(name):\n",
    "        return str(Path(DATADIR, \"{}.sentences.txt\".format(name)))\n",
    "\n",
    "    def ftags(name):\n",
    "        return str(Path(DATADIR, \"{}.labels.txt\".format(name)))\n",
    "\n",
    "    # Estimator, train and evaluate\n",
    "    train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
    "                                   params, shuffle_and_repeat=True)\n",
    "    eval_inpf = functools.partial(input_fn, fwords('test'), ftags('test'))\n",
    "\n",
    "    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
    "    estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
    "    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True) # TODO take this outside?\n",
    "    # hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
    "    #     estimator, 'f1', 500, min_steps=8000, run_every_secs=120)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf) #, hooks=[hook])\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    # # Write predictions to file\n",
    "    # def write_predictions(name):\n",
    "    #     Path('results/score').mkdir(parents=True, exist_ok=True)\n",
    "    #     with Path('results/score/{}.preds.txt'.format(name)).open('wb') as f:\n",
    "    #         test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n",
    "    #         golds_gen = generator_fn(fwords(name), ftags(name))\n",
    "    #         preds_gen = estimator.predict(test_inpf)\n",
    "    #         for golds, preds in zip(golds_gen, preds_gen):\n",
    "    #             ((words, _), tags) = golds\n",
    "    #             for word, tag, tag_pred in zip(words, tags, preds['tags']):\n",
    "    #                 f.write(b' '.join([word, tag, tag_pred]) + b'\\n')\n",
    "    #             f.write(b'\\n')\n",
    "\n",
    "    for name in ['train', 'test']:\n",
    "        write_predictions(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '../data/processed_data/gmb'\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "    # Encode in Bytes for TF\n",
    "    words = [w.encode() for w in line_words.strip().split()]\n",
    "    tags = [t.encode() for t in line_tags.strip().split()]\n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "    return (words, len(words)), tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "\n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None], ()), [None])\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    defaults = (('<pad>', 0), 'O')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_shapes=shapes, output_types=types)\n",
    "\n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    dataset = (dataset\n",
    "               .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "params = {\n",
    "    \"dim\": 300,\n",
    "    \"dropout\": 0.5,\n",
    "    # \"num_oov_buckets\": 1,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer\": 15000,\n",
    "    \"lstm_size\": 100,\n",
    "    \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "    \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "}\n",
    "\n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "# Estimator, train and evaluate\n",
    "train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
    "                               params, shuffle_and_repeat=True)\n",
    "eval_inpf = functools.partial(input_fn, fwords('test'), ftags('test'))\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "\n",
    "embeddings_size=50\n",
    "vocabulary_size=100\n",
    "dropout=0.2\n",
    "lstm_units=100\n",
    "labels_size=10\n",
    "\n",
    "input_word = Input(shape=(embeddings_size,))\n",
    "model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "model = SpatialDropout1D(dropout)(model)\n",
    "model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "words,tags=fwords('train'), ftags('train')\n",
    "shapes = (([None], ()), [None])\n",
    "types = ((tf.string, tf.int32), tf.string)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    functools.partial(generator_fn, words, tags),\n",
    "    output_shapes=shapes, output_types=types)\n",
    "\n",
    "\n",
    "model.fit(x=train_inpf,\n",
    "          y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          callbacks=[self.checkpoint, EarlyStopping()],\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# shapes = ([None], [None])\n",
    "# types = (tf.string, tf.string)\n",
    "# defaults = ('<pad>''O')\n",
    "\n",
    "# def generator_fn(words, tags):\n",
    "#     with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "#         for line_words, line_tags in zip(f_words, f_tags):\n",
    "#             yield parse_fn(line_words, line_tags)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_generator(\n",
    "#     functools.partial(generator_fn, words, tags),\n",
    "#     output_shapes=shapes, output_types=types)\n",
    "\n",
    "\n",
    "# dataset = (dataset.padded_batch(32, shapes, defaults).prefetch(1))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "            \n",
    "            \n",
    "def gen_train_series(sentences, labels):\n",
    "    for sentence, label in sentences, labels:\n",
    "        yield sentence, label\n",
    "\n",
    "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for a in functools.partial(generator_fn, words, tags)():\n",
    "    print(a)\n",
    "    break\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<PAD>': '0',\n",
       " '<UNK>': '1',\n",
       " 'O': 2,\n",
       " 'I-TTL': 3,\n",
       " 'I-TIM': 4,\n",
       " 'I-PER': 5,\n",
       " 'I-PCT': 6,\n",
       " 'I-ORG': 7,\n",
       " 'I-MON': 8,\n",
       " 'I-LOC': 9,\n",
       " 'I-DAT': 10,\n",
       " 'I-ART': 11}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Path(params['tags']).open() as f:\n",
    "    tag2idx = {\n",
    "        \"<PAD>\": \"0\",\n",
    "        \"<UNK>\": \"1\"\n",
    "    }    \n",
    "    for i, t in enumerate(f,2):\n",
    "        tag2idx[t.strip()] = i\n",
    "        print(i)\n",
    "#     _ = [tag2idx[t.strip()] = i for i, t in enumerate(f,2)]\n",
    "\n",
    "tag2idx\n",
    "    \n",
    "# for i, t in enumerate(f,2):\n",
    "#     print(t)\n",
    "# _ = [tag2idx[t.strip()] = i for i, t in enumerate(f,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embeddings_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-4a20c6951c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0minput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_len\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0minput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_len\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lstm_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embeddings_dim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from pathlib import Path\n",
    "DATADIR = '../data/processed_data/gmb'\n",
    "import tensorflow as tf\n",
    "\n",
    "def tags_dictionaries():\n",
    "    with Path(params['tags']).open() as f:\n",
    "        tag2idx = {t.strip(): i for i, t in enumerate(f)} \n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "def words_dictionaries():\n",
    "    with Path(params['words']).open() as f:\n",
    "        word2idx = {t.strip(): i for i, t in enumerate(f,1)}\n",
    "        word2idx[\"UNK\"]=0\n",
    "        word2idx[\"ENDPAD\"]=len(word2idx)\n",
    "        idx2word = {i: t for t, i in word2idx.items()}\n",
    "    return word2idx, idx2word, len(word2idx)\n",
    "        \n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "#     words = [w.encode() for w in line_words.strip().split()]\n",
    "#     tags = [t.encode() for t in line_tags.strip().split()]\n",
    "#     print(line_words.strip().split())\n",
    "    words = np.array([word2idx.get(w, 0) for w in line_words.strip().split()])\n",
    "    tags = np.array([tag2idx[t] for t in line_tags.strip().split()])\n",
    "    \n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "#     return (words, len(words)), tags\n",
    "    return words, tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "                        \n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    shapes = (tf.TensorShape([None]),tf.TensorShape([None]))        \n",
    "    dataset = (dataset\n",
    "               .padded_batch(batch_size=32, \n",
    "                             padded_shapes=([params[\"max_len\"]], [params[\"max_len\"]]), \n",
    "                             padding_values=(params['vocab_size']-1,params['pad_index'])\n",
    "                            )\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "params = {\n",
    "    \"dim\": 100,\n",
    "    \"dropout\": 0.5,\n",
    "    # \"num_oov_buckets\": 1,\n",
    "    \"max_len\": 60,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer\": 15000,\n",
    "    \"lstm_size\": 100,\n",
    "    \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "    \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "}\n",
    "\n",
    "tag2idx, idx2tag = tags_dictionaries()\n",
    "word2idx, idx2word, vocab_size = words_dictionaries()\n",
    "\n",
    "params['vocab_size']=vocab_size\n",
    "params['pad_index']=tag2idx['O']\n",
    "\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "\n",
    "embeddings_size=50\n",
    "# vocabulary_size=100\n",
    "dropout=0.2\n",
    "lstm_units=100\n",
    "labels_size=10\n",
    "\n",
    "\n",
    "# input_word = Input(shape=(params[\"max_len\"],))\n",
    "# model = Embedding(input_dim=params['vocab_size'], output_dim=embeddings_size, input_length=params[\"max_len\"])(input_word)\n",
    "# model = SpatialDropout1D(dropout)(model)\n",
    "# model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "# out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "# model = Model(input_word, out)\n",
    "# model.compile(optimizer=\"adam\",\n",
    "#               loss=\"sparse_categorical_crossentropy\",\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# from keras_crf import CRF\n",
    "\n",
    "# input_word = Input(shape=(params[\"max_len\"],))\n",
    "# sequence_mask = tf.keras.layers.Lambda(lambda x: tf.greater(x, 0))(input_word)\n",
    "# model = Embedding(input_dim=params['vocab_size'], output_dim=embeddings_size, input_length=params[\"max_len\"])(input_word)\n",
    "# model = SpatialDropout1D(dropout)(model)\n",
    "# model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "# logits = Dense(labels_size, activation=\"softmax\")(model)\n",
    "\n",
    "# # crf = CRF(params[\"max_len\"])\n",
    "# crf = CRF(len(idx2tag))\n",
    "# outputs = crf(logits, mask=sequence_mask)\n",
    "# model = Model(input_word, outputs)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\", \n",
    "#     loss=crf.neg_log_likelihood,\n",
    "#     metrics=[crf.accuracy]\n",
    "#     )\n",
    "# model.summary()\n",
    "\n",
    "# from keras_crf import CRF\n",
    "\n",
    "# sequence_input = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='sequence_input')\n",
    "# sequence_mask = tf.keras.layers.Lambda(lambda x: tf.greater(x, 0))(sequence_input)\n",
    "# outputs = tf.keras.layers.Embedding(21128, 128)(sequence_input)\n",
    "# outputs = tf.keras.layers.Dense(256)(outputs)\n",
    "# crf = CRF(7)\n",
    "# # mask is important to compute sequence length in CRF\n",
    "# outputs = crf(outputs, mask=sequence_mask)\n",
    "# model = tf.keras.Model(inputs=sequence_input, outputs=outputs)\n",
    "# model.compile(\n",
    "#     loss=crf.neg_log_likelihood,\n",
    "#     metrics=[\n",
    "#         crf.accuracy\n",
    "#     ],\n",
    "#     optimizer=tf.keras.optimizers.Adam(4e-5)\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "input_word = Input(shape=(params[\"max_len\"],))\n",
    "input_mask = Lambda(lambda x: tf.greater(x, 0))(input_word)\n",
    "model = Embedding(input_dim=params['vocab_size'], output_dim=params[\"embeddings_dim\"], input_length=params[\"max_len\"])(input_word)\n",
    "model = SpatialDropout1D(params[\"dropout\"])(model)\n",
    "model = Bidirectional(LSTM(units=params[\"lstm_size\"], return_sequences=True, recurrent_dropout=params[\"dropout\"]))(model)\n",
    "logits = Dense(params[\"labels_size\"], activation=None)(model)\n",
    "crf = CRF(params[\"labels_size\"])\n",
    "outputs = crf(logits, mask=input_mask)\n",
    "model = tf.keras.Model(inputs=input_word, outputs=outputs)\n",
    "model.compile(loss=crf.neg_log_likelihood, metrics=[crf.accuracy], optimizer='adam')\n",
    "    \n",
    "    \n",
    "# optimizer = tf.keras.optimizers.Adam(4e-5)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=crf.loss, metrics=[crf.accuracy])\n",
    "\n",
    "\n",
    "dataset = functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 31s 71ms/step - loss: 31.7135 - accuracy: 0.8808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc175921ac8>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from pathlib import Path\n",
    "DATADIR = '../data/processed_data/gmb'\n",
    "import tensorflow as tf\n",
    "\n",
    "def tags_dictionaries():\n",
    "    with Path(params['tags']).open() as f:\n",
    "        tag2idx = {t.strip(): i for i, t in enumerate(f)} \n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "    return tag2idx, idx2tag, len(tag2idx)\n",
    "\n",
    "def words_dictionaries():\n",
    "    with Path(params['words']).open() as f:\n",
    "        word2idx = {t.strip(): i for i, t in enumerate(f,1)}\n",
    "        word2idx[\"UNK\"]=0\n",
    "        word2idx[\"ENDPAD\"]=len(word2idx)\n",
    "        idx2word = {i: t for t, i in word2idx.items()}\n",
    "    return word2idx, idx2word, len(word2idx)\n",
    "        \n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "    words = np.array([word2idx.get(w, 0) for w in line_words.strip().split()])\n",
    "    tags = np.array([tag2idx[t] for t in line_tags.strip().split()])\n",
    "    \n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "    return words, tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "                        \n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    shapes = (tf.TensorShape([None]),tf.TensorShape([None]))        \n",
    "    dataset = (dataset\n",
    "               .padded_batch(batch_size=32, \n",
    "                             padded_shapes=([params[\"max_len\"]], [params[\"max_len\"]]), \n",
    "                             padding_values=(params['vocab_size']-1,params['pad_index'])\n",
    "                            )\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "params = {\n",
    "    \"dim\": 100,\n",
    "    \"dropout\": 0.5,\n",
    "    # \"num_oov_buckets\": 1,\n",
    "    \"max_len\": 60,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer\": 15000,\n",
    "    \"lstm_size\": 100,\n",
    "    \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "    \"tags\": str(Path(DATADIR, \"tags.txt\")),\n",
    "    \"embeddings_dim\": 50\n",
    "}\n",
    "\n",
    "tag2idx, idx2tag, tags_len = tags_dictionaries()\n",
    "word2idx, idx2word, vocab_size = words_dictionaries()\n",
    "\n",
    "params['vocab_size']=vocab_size\n",
    "params['pad_index']=tag2idx['O']\n",
    "params[\"labels_size\"]=tags_len\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "\n",
    "embeddings_size=50\n",
    "dropout=0.2\n",
    "lstm_units=100\n",
    "labels_size=10\n",
    "\n",
    "input_word = Input(shape=(params[\"max_len\"],))\n",
    "input_mask = Lambda(lambda x: tf.greater(x, 0))(input_word)\n",
    "model = Embedding(input_dim=params['vocab_size'], output_dim=params[\"embeddings_dim\"], input_length=params[\"max_len\"])(input_word)\n",
    "model = SpatialDropout1D(params[\"dropout\"])(model)\n",
    "model = Bidirectional(LSTM(units=params[\"lstm_size\"], return_sequences=True, recurrent_dropout=params[\"dropout\"]))(model)\n",
    "logits = Dense(params[\"labels_size\"], activation=None)(model)\n",
    "crf = CRF(params[\"labels_size\"])\n",
    "outputs = crf(logits, mask=input_mask)\n",
    "model = tf.keras.Model(inputs=input_word, outputs=outputs)\n",
    "model.compile(loss=crf.neg_log_likelihood, metrics=[crf.accuracy], optimizer='adam')\n",
    "\n",
    "dataset = functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc1ba71e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = functools.partial(input_fn, fwords('test'), ftags('test'), params, shuffle_and_repeat=False)()\n",
    "predictions = model.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           True \t Pred\n",
      "\n",
      "------------------------------\n",
      "\"The           O\tO\n",
      "demonstrators  O\tO\n",
      "Sunday         I-DAT\tI-LOC\n",
      "called         O\tO\n",
      "on             O\tO\n",
      "the            O\tO\n",
      "government     O\tO\n",
      "to             O\tO\n",
      "ease           O\tO\n",
      "restrictions   O\tO\n",
      "on             O\tO\n",
      "political      O\tO\n",
      "activities     O\tO\n",
      "at             O\tO\n",
      "universities   O\tO\n",
      ",              O\tO\n",
      "and            O\tO\n",
      "to             O\tO\n",
      "allow          O\tO\n",
      "free           O\tO\n",
      "and            O\tO\n",
      "fair           O\tO\n",
      "student        O\tO\n",
      "elections      O\tO\n",
      ".\"             O\tO\n"
     ]
    }
   ],
   "source": [
    "with open(fwords('test')) as f:\n",
    "    test_words = f.readlines()\n",
    "    \n",
    "with open(ftags('test')) as f:\n",
    "    test_tags = f.readlines()    \n",
    "\n",
    "i = np.random.randint(0, predictions.shape[0]) #659\n",
    "# i = 0\n",
    "words = test_words[i].split()\n",
    "tags = test_tags[i].split()\n",
    "p = np.argmax(predictions[i], axis=-1)\n",
    "# y_true = y_test[i]\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"-\" *30)\n",
    "for word, true_tag, pred in zip(words, tags, p[:len(words)]):\n",
    "    print(\"{:15}{}\\t{}\".format(word, true_tag, idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-DAT       0.42      0.32      0.36       566\n",
      "       I-LOC       0.46      0.53      0.49       771\n",
      "       I-MON       0.00      0.00      0.00        44\n",
      "       I-ORG       0.40      0.04      0.07       654\n",
      "       I-PCT       0.00      0.00      0.00        23\n",
      "       I-PER       0.46      0.03      0.05       423\n",
      "       I-TIM       0.00      0.00      0.00        13\n",
      "       I-TTL       0.00      0.00      0.00         1\n",
      "           O       0.93      1.00      0.96     14121\n",
      "\n",
      "    accuracy                           0.88     16616\n",
      "   macro avg       0.29      0.21      0.21     16616\n",
      "weighted avg       0.85      0.88      0.85     16616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lievgarcia/Documents/nlp_specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lievgarcia/Documents/nlp_specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lievgarcia/Documents/nlp_specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "y_true, y_pred = [],[]\n",
    "target_names = list(idx2tag.values())\n",
    "\n",
    "def write_predictions(name):\n",
    "    Path('results/score').mkdir(parents=True, exist_ok=True)\n",
    "    with open('results/score/{}.preds.csv'.format(name), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        golds_gen = generator_fn(fwords(name), ftags(name))\n",
    "        dataset = functools.partial(input_fn, fwords(name), ftags(name), params, shuffle_and_repeat=False)()\n",
    "        preds_gen = model.predict(dataset)        \n",
    "        for golds, preds in zip(golds_gen, preds_gen):\n",
    "            \n",
    "            (words, tags) = golds\n",
    "            preds = np.argmax(preds, axis=-1)[:len(words)]\n",
    "            for word, tag, tag_pred in zip(words, tags, preds):\n",
    "                y_true.append(idx2tag[tag])\n",
    "                y_pred.append(idx2tag[tag_pred])\n",
    "                writer.writerow([idx2word[word], idx2tag[tag], idx2tag[tag_pred]])\n",
    "            \n",
    "            \n",
    "            \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for name in ['test']:\n",
    "    write_predictions(name)    \n",
    "    \n",
    "print(classification_report(y_true, y_pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names = list(idx2tag.values())\n",
    "# # classification_report(y_true, y_pred, target_names=target_names)\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true\n",
    "# from collections import Counter\n",
    "# Counter(y_true)\n",
    "# # print(Counter(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # tag2idx, idx2tag\n",
    "# # word2idx, idx2word, vocab_size\n",
    "# # word2idx\n",
    "\n",
    "# word2num = word2idx\n",
    "# label2ner = idx2tag\n",
    "# max_len = 60\n",
    "\n",
    "# in_sentence = \"Tigray , Amhara and Oromia regions .\"\n",
    "# words = in_sentence.split()\n",
    "\n",
    "# sentence = []\n",
    "# for i in words:\n",
    "#     if i in word2num:\n",
    "#         sentence.append(word2num[i])\n",
    "#     else:\n",
    "#         sentence.append(word2num['UNK'])\n",
    "# print(sentence)\n",
    "# print(\"\\n\")\n",
    "# sentence = pad_sequences([sentence], maxlen=max_len, value=vocab_size-1)\n",
    "# y_pred = model.predict(sentence)\n",
    "# # print(y_pred[0])\n",
    "# print(y_pred[1][0])\n",
    "# # y_pred = model.predict(sentence).argmax(-1)[sentence > 0]\n",
    "# # ner_dict = {\n",
    "# #     \"PER\": '',\n",
    "# #     \"LOC\": '',\n",
    "# #     \"ORG\": '',\n",
    "# #     \"O\": ''\n",
    "# # }\n",
    "# # ner_list = {\n",
    "# #     \"PER\": [],\n",
    "# #     \"LOC\": [],\n",
    "# #     \"ORG\": [],\n",
    "# #     \"O\": []\n",
    "# # }\n",
    "# # print(y_pred)\n",
    "# # for i in range(len(words)):\n",
    "# #     ner = label2ner[y_pred[i]][-3:]\n",
    "# #     ner_dict[ner] += words[i]\n",
    "# #     for n, s in ner_dict.items():\n",
    "# #         if n != ner and s:\n",
    "# #             ner_list[n].append(s)\n",
    "# #             ner_dict[n] = ''\n",
    "\n",
    "# # print(\"predict result: {}\".format(ner_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(predictions.shape[0]):\n",
    "    for token in predictions[i]:\n",
    "        if np.argmax(token)>0:\n",
    "            print(np.argmax(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 60), dtype=int32, numpy=\n",
      "array([[1204,   95,  212, ..., 9232, 9232, 9232],\n",
      "       [2573, 4046,  331, ..., 9232, 9232, 9232],\n",
      "       [1078,  366,  331, ..., 9232, 9232, 9232],\n",
      "       ...,\n",
      "       [   0,   32,  873, ..., 9232, 9232, 9232],\n",
      "       [   0, 2937,   23, ..., 9232, 9232, 9232],\n",
      "       [  41,  585,  586, ..., 9232, 9232, 9232]], dtype=int32)>, <tf.Tensor: shape=(32, 60), dtype=int32, numpy=\n",
      "array([[7, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 5, 5, ..., 0, 0, 0]], dtype=int32)>)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset = functools.partial(input_fn, fwords('test'), ftags('test'), params, shuffle_and_repeat=True)()\n",
    "i=0\n",
    "for a in dataset:\n",
    "    print(a)\n",
    "    break\n",
    "    i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0][4].numpy()\n",
    "[idx2word[t] for t in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2word.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word2idx.items())[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "\n",
    "words = fwords('train')\n",
    "tags = ftags('train')\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=([None]), dtype=tf.int32))\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    functools.partial(generator_fn, words, tags),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    if len(i[0])> 50:\n",
    "        print(len(i[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_tagger",
   "language": "python",
   "name": "ner_tagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
