{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras import Model, Input\n",
    "# from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "# from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "import json\n",
    "import functools\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# path = Path(__file__).parent\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\".\")\n",
    "# from datasets.dataset import Dataset\n",
    "\n",
    "# class LSTMCRF:\n",
    "\n",
    "#     def __init__(self, vocabulary_size, labels_size, embeddings_size=50, lstm_units=100, dropout=0.2):\n",
    "#         input_word = Input(shape=(embeddings_size,))\n",
    "#         model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "#         model = SpatialDropout1D(dropout)(model)\n",
    "#         model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "#         out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "#         self.model = Model(input_word, out)\n",
    "#         self.checkpoint = ModelCheckpoint(\"{}/model.h5\".format(path), monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "\n",
    "#     def fit(self, data):\n",
    "#         self.model.compile(optimizer=\"adam\",\n",
    "#             loss=\"sparse_categorical_crossentropy\",\n",
    "#             metrics=[\"accuracy\"])\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = data.Xy\n",
    "\n",
    "#         self.model.fit(\n",
    "#             x=X_train,\n",
    "#             y=y_train,\n",
    "#             validation_data=(X_test,y_test),\n",
    "#             batch_size=32,\n",
    "#             epochs=3,\n",
    "#             callbacks=[self.checkpoint, EarlyStopping()],\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#     def test(self, data):\n",
    "#         _, X, _, y = data.Xy\n",
    "#         self.model.evaluate(X, y)\n",
    "#         n_sequences, sequence_len = X.shape\n",
    "#         predictions = self.model.predict(X)\n",
    "#         flat_predictions = []\n",
    "#         flat_gold = []\n",
    "#         with open(\"{}/predictions.txt\".format(path), \"w\") as f:\n",
    "#             f.write(f\"WORD\\tPREDICTION\\tGOLD STANDARD\\n\")\n",
    "#             for sequence_i in range(n_sequences):\n",
    "#                 f.write(\"\\n\")\n",
    "#                 for word_i in range(sequence_len):\n",
    "#                     word = data.idx2word[X[sequence_i, word_i]]\n",
    "#                     prediction_i = np.argmax(predictions[sequence_i, word_i, :])\n",
    "#                     prediction = data.idx2tag[prediction_i]\n",
    "#                     flat_predictions.append(prediction)\n",
    "#                     truth = data.idx2tag[y[sequence_i, word_i]]\n",
    "#                     flat_gold.append(truth)\n",
    "#                     f.write(f\"{word}\\t{prediction}\\t{truth}\\n\")\n",
    "#         with open(\"{}/metric-report.txt\".format(path), \"w\") as f:\n",
    "#             report = classification_report(flat_gold, flat_predictions)\n",
    "#             f.write(report)\n",
    "\n",
    "#     def save(self):\n",
    "#         # serialize model to JSON\n",
    "#         model_json = self.model.to_json()\n",
    "#         with open(\"{}/model.json\".format(path), \"w\") as json_file:\n",
    "#             json_file.write(model_json)\n",
    "#         # serialize weights to HDF5\n",
    "#         self.model.save_weights(\"{}/model.h5\".format(path))\n",
    "#         print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset = Dataset(path=\"./data/processed_data/gmb-1.0.0.csv\", maxlen=50)\n",
    "#     model = LSTMCRF(dataset.vocabulary_size, dataset.labels_size)\n",
    "#     model.fit(dataset)\n",
    "#     model.test(dataset)\n",
    "#     model.save()\n",
    "#     # print(path)\n",
    "\n",
    "\n",
    "DATADIR = './data/processed_data/gmb'\n",
    "\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "    # Encode in Bytes for TF\n",
    "    words = [w.encode() for w in line_words.strip().split()]\n",
    "    tags = [t.encode() for t in line_tags.strip().split()]\n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "    return (words, len(words)), tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "\n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None], ()), [None])\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    defaults = (('<pad>', 0), 'O')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_shapes=shapes, output_types=types)\n",
    "\n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    dataset = (dataset\n",
    "               .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    input_word = Input(shape=(embeddings_size,))\n",
    "    model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "    model = SpatialDropout1D(dropout)(model)\n",
    "    model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "    out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "    self.model = Model(input_word, out)\n",
    "    self.checkpoint = ModelCheckpoint(\"results/model.h5\", monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, model_dir=model_dir)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = {\n",
    "        \"dim\": 300,\n",
    "        \"dropout\": 0.5,\n",
    "        # \"num_oov_buckets\": 1,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer\": 15000,\n",
    "        \"lstm_size\": 100,\n",
    "        \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "        \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "    }\n",
    "\n",
    "    with Path(\"results/params.json\").open(\"w\") as f:\n",
    "        json.dump(params, f, indent=4, sort_keys=True)\n",
    "\n",
    "    def fwords(name):\n",
    "        return str(Path(DATADIR, \"{}.sentences.txt\".format(name)))\n",
    "\n",
    "    def ftags(name):\n",
    "        return str(Path(DATADIR, \"{}.labels.txt\".format(name)))\n",
    "\n",
    "    # Estimator, train and evaluate\n",
    "    train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
    "                                   params, shuffle_and_repeat=True)\n",
    "    eval_inpf = functools.partial(input_fn, fwords('test'), ftags('test'))\n",
    "\n",
    "    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n",
    "    estimator = tf.estimator.Estimator(model_fn, 'results/model', cfg, params)\n",
    "    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True) # TODO take this outside?\n",
    "    # hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
    "    #     estimator, 'f1', 500, min_steps=8000, run_every_secs=120)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf) #, hooks=[hook])\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    # # Write predictions to file\n",
    "    # def write_predictions(name):\n",
    "    #     Path('results/score').mkdir(parents=True, exist_ok=True)\n",
    "    #     with Path('results/score/{}.preds.txt'.format(name)).open('wb') as f:\n",
    "    #         test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n",
    "    #         golds_gen = generator_fn(fwords(name), ftags(name))\n",
    "    #         preds_gen = estimator.predict(test_inpf)\n",
    "    #         for golds, preds in zip(golds_gen, preds_gen):\n",
    "    #             ((words, _), tags) = golds\n",
    "    #             for word, tag, tag_pred in zip(words, tags, preds['tags']):\n",
    "    #                 f.write(b' '.join([word, tag, tag_pred]) + b'\\n')\n",
    "    #             f.write(b'\\n')\n",
    "\n",
    "    for name in ['train', 'test']:\n",
    "        write_predictions(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e6ee7917dbd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m model.fit(x=train_inpf,\n\u001b[0;32m---> 93\u001b[0;31m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "DATADIR = '../data/processed_data/gmb'\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "    # Encode in Bytes for TF\n",
    "    words = [w.encode() for w in line_words.strip().split()]\n",
    "    tags = [t.encode() for t in line_tags.strip().split()]\n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "    return (words, len(words)), tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "\n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    shapes = (([None], ()), [None])\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    defaults = (('<pad>', 0), 'O')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_shapes=shapes, output_types=types)\n",
    "\n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    dataset = (dataset\n",
    "               .padded_batch(params.get('batch_size', 20), shapes, defaults)\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "params = {\n",
    "    \"dim\": 300,\n",
    "    \"dropout\": 0.5,\n",
    "    # \"num_oov_buckets\": 1,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer\": 15000,\n",
    "    \"lstm_size\": 100,\n",
    "    \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "    \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "}\n",
    "\n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "# Estimator, train and evaluate\n",
    "train_inpf = functools.partial(input_fn, fwords('train'), ftags('train'),\n",
    "                               params, shuffle_and_repeat=True)\n",
    "eval_inpf = functools.partial(input_fn, fwords('test'), ftags('test'))\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "\n",
    "embeddings_size=50\n",
    "vocabulary_size=100\n",
    "dropout=0.2\n",
    "lstm_units=100\n",
    "labels_size=10\n",
    "\n",
    "input_word = Input(shape=(embeddings_size,))\n",
    "model = Embedding(input_dim=vocabulary_size, output_dim=embeddings_size)(input_word)\n",
    "model = SpatialDropout1D(dropout)(model)\n",
    "model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "words,tags=fwords('train'), ftags('train')\n",
    "shapes = (([None], ()), [None])\n",
    "types = ((tf.string, tf.int32), tf.string)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    functools.partial(generator_fn, words, tags),\n",
    "    output_shapes=shapes, output_types=types)\n",
    "\n",
    "\n",
    "model.fit(x=train_inpf,\n",
    "          y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          callbacks=[self.checkpoint, EarlyStopping()],\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FlatMapDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a3fde58ced1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'FlatMapDataset' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None), (None, None)), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# shapes = ([None], [None])\n",
    "# types = (tf.string, tf.string)\n",
    "# defaults = ('<pad>''O')\n",
    "\n",
    "# def generator_fn(words, tags):\n",
    "#     with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "#         for line_words, line_tags in zip(f_words, f_tags):\n",
    "#             yield parse_fn(line_words, line_tags)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_generator(\n",
    "#     functools.partial(generator_fn, words, tags),\n",
    "#     output_shapes=shapes, output_types=types)\n",
    "\n",
    "\n",
    "# dataset = (dataset.padded_batch(32, shapes, defaults).prefetch(1))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "            \n",
    "            \n",
    "def gen_train_series(sentences, labels):\n",
    "    for sentence, label in sentences, labels:\n",
    "        yield sentence, label\n",
    "\n",
    "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([b'Ethiopia', b'has', b'reported', b'18', b'new', b'cases', b'of', b'polio', b'as', b'it', b'begins', b'a', b'nationwide', b'vaccination', b'program', b'targeting', b'more', b'than', b'16', b'million', b'children', b'under', b'the', b'age', b'of', b'five', b'.'], 27), [b'I-ORG', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O', b'O'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4b5f4010c595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for a in functools.partial(generator_fn, words, tags)():\n",
    "    print(a)\n",
    "    break\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9233\n",
      "Epoch 1/3\n",
      "316/316 [==============================] - 24s 61ms/step - loss: 0.4995 - accuracy: 0.9299\n",
      "Epoch 2/3\n",
      " 63/316 [====>.........................] - ETA: 15s - loss: 0.1125 - accuracy: 0.9605"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d6c497a58638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_and_repeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/nlp specialisation/ner_tagger/ner_tagger/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from pathlib import Path\n",
    "DATADIR = '../data/processed_data/gmb'\n",
    "import tensorflow as tf\n",
    "\n",
    "def tags_dictionaries():\n",
    "    with Path(params['tags']).open() as f:\n",
    "        tag2idx = {t.strip(): i for i, t in enumerate(f)} \n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "def words_dictionaries():\n",
    "    with Path(params['words']).open() as f:\n",
    "        word2idx = {t.strip(): i for i, t in enumerate(f,1)}\n",
    "        word2idx[\"UNK\"]=0\n",
    "        word2idx[\"ENDPAD\"]=len(word2idx)\n",
    "        idx2word = {i: t for t, i in word2idx.items()}\n",
    "    return word2idx, idx2word, len(word2idx)\n",
    "        \n",
    "def fwords(name):\n",
    "    return str(Path(DATADIR, \"{}.sentences.csv\".format(name)))\n",
    "\n",
    "def ftags(name):\n",
    "    return str(Path(DATADIR, \"{}.labels.csv\".format(name)))\n",
    "\n",
    "def parse_fn(line_words, line_tags):\n",
    "#     words = [w.encode() for w in line_words.strip().split()]\n",
    "#     tags = [t.encode() for t in line_tags.strip().split()]\n",
    "#     print(line_words.strip().split())\n",
    "    words = np.array([word2idx.get(w, 0) for w in line_words.strip().split()])\n",
    "    tags = np.array([tag2idx[t] for t in line_tags.strip().split()])\n",
    "    \n",
    "    assert len(words) == len(tags), \"Words and tags lengths don't match\"\n",
    "#     return (words, len(words)), tags\n",
    "    return words, tags\n",
    "\n",
    "def generator_fn(words, tags):\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            yield parse_fn(line_words, line_tags)\n",
    "                        \n",
    "def input_fn(words, tags, params=None, shuffle_and_repeat=False):\n",
    "    params = params if params is not None else {}\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=([None]), dtype=tf.int32))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(generator_fn, words, tags),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle_and_repeat:\n",
    "        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\n",
    "\n",
    "    shapes = (tf.TensorShape([None]),tf.TensorShape([None]))        \n",
    "    dataset = (dataset\n",
    "               .padded_batch(batch_size=32, \n",
    "                             padded_shapes=([params[\"max_len\"]], [params[\"max_len\"]]), \n",
    "                             padding_values=(params['vocab_size']-1,params['pad_index'])\n",
    "                            )\n",
    "               .prefetch(1))\n",
    "    return dataset\n",
    "\n",
    "params = {\n",
    "    \"dim\": 100,\n",
    "    \"dropout\": 0.5,\n",
    "    # \"num_oov_buckets\": 1,\n",
    "    \"max_len\": 60,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer\": 15000,\n",
    "    \"lstm_size\": 100,\n",
    "    \"words\": str(Path(DATADIR, \"vocabulary.txt\")),\n",
    "    \"tags\": str(Path(DATADIR, \"tags.txt\"))\n",
    "}\n",
    "\n",
    "tag2idx, idx2tag = tags_dictionaries()\n",
    "word2idx, idx2word, vocab_size = words_dictionaries()\n",
    "\n",
    "params['vocab_size']=vocab_size\n",
    "params['pad_index']=tag2idx['O']\n",
    "\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "\n",
    "embeddings_size=50\n",
    "# vocabulary_size=100\n",
    "dropout=0.2\n",
    "lstm_units=100\n",
    "labels_size=10\n",
    "\n",
    "\n",
    "input_word = Input(shape=(params[\"max_len\"],))\n",
    "model = Embedding(input_dim=params['vocab_size'], output_dim=embeddings_size, input_length=params[\"max_len\"])(input_word)\n",
    "model = SpatialDropout1D(dropout)(model)\n",
    "model = Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout))(model)\n",
    "out = TimeDistributed(Dense(labels_size, activation=\"softmax\"))(model)\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "dataset = functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     for _ in functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)():\n",
    "#         pass\n",
    "# #     break\n",
    "# # a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 60), dtype=int32, numpy=\n",
      "array([[1039,  189,  331, ..., 9232, 9232, 9232],\n",
      "       [1079,  387, 8058, ..., 9232, 9232, 9232],\n",
      "       [  41,   99, 1063, ..., 9232, 9232, 9232],\n",
      "       ...,\n",
      "       [  41,  692, 4638, ..., 9232, 9232, 9232],\n",
      "       [  41,  277, 3072, ..., 9232, 9232, 9232],\n",
      "       [   0, 5189, 5190, ..., 9232, 9232, 9232]], dtype=int32)>, <tf.Tensor: shape=(32, 60), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 3, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 5, 5, ..., 0, 0, 0]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "dataset = functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "for a in dataset:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Kremlin',\n",
       " 'Friday',\n",
       " 'said',\n",
       " 'Mr.',\n",
       " 'Blair',\n",
       " 'expressed',\n",
       " 'regret',\n",
       " 'that',\n",
       " 'intensive',\n",
       " 'work',\n",
       " 'on',\n",
       " 'forming',\n",
       " 'a',\n",
       " 'new',\n",
       " 'Cabinet',\n",
       " 'would',\n",
       " 'prevent',\n",
       " 'him',\n",
       " 'from',\n",
       " 'attending',\n",
       " 'the',\n",
       " 'ceremonies',\n",
       " 'Monday',\n",
       " '.',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD',\n",
       " 'ENDPAD']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[0][4].numpy()\n",
    "[idx2word[t] for t in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2word.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('survive', 9231)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2idx.items())[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functools.partial(input_fn, fwords('train'), ftags('train'), params, shuffle_and_repeat=True)()\n",
    "\n",
    "words = fwords('train')\n",
    "tags = ftags('train')\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=([None]), dtype=tf.int32))\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    functools.partial(generator_fn, words, tags),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "55\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    if len(i[0])> 50:\n",
    "        print(len(i[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5e2c37702258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_tagger",
   "language": "python",
   "name": "ner_tagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
